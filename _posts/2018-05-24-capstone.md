---
layout: post
title: "DSI3 Capstone - Tweetful of Sentiments"
excerpt_separator: "<!--more-->"
categories:
  - Post Formats
tags:
  - Post Formats
  - readability
  - standard
---

__Introduction__ 
For my capstone, I chose to work on sentiment classification of Twitter posts. The decision is driven by two main factors: the first is that much of the reputation  

Challenges to applying natural language processing to Twitter posts. 


The objective is to reach a 70% accuracy in labelling the sentiment of tweets. 

__Dataset__ 
I used a dataset produced by Spanish researchers  (http://nlp.uned.es/replab2013/) for a academic conference held in 2013. The dataset is prepared by the Natural Language Processing and Information Retrieval Group at UNED. It contains more than 100,000 tweets in English and Spanish, collected over six months for more than 60 entities (across a broad spectrum including companies, brands, universities and music artistes). The UNED researchers got subject matter experts for each domain (e.g. corporate, public agencies etc) to examine each tweet and provide it with a label of Positive, Neutral or Negative. Out of the 100,000 tweets, I extracted 40,000 tweets for training and testing of my models. 

<img src="https://github.com/hankelvin/hankelvin.github.io/blob/master/_screenshots/tweets%20dataset.png">

__General approach__ 
Before starting, I conducted extensive research to to get an overview of the available tools and approaches for social media sentiment analysis.  This covered articles, book chapters, academic papers, blog and forum posts. A very useful resource is Speech and Language Processing by Stanford University’s Dan Jurafsky and James H. Martin (a draft copy of the upcoming 3rd edition can be found at: https://web.stanford.edu/~jurafsky/slp3/). 

I identified a few possible approaches - these included bag of words, machine learning, using off-the-shelf NLP packages like Spacy, Textblob etc. In the end, I decided to utilise word embeddings methods as they are state-of-the-art currently and are known to leverage vectorising of text within a corpus such that all of the tokens in the corpus, given their position in vector space, retain their semantic and syntactic meaning as well as retain the relationships between words and words pairs. The following diagram is frequently used to illustrate the properties of a word embeddings model. 

<img style="-webkit-user-select: none;" src="https://github.com/hankelvin/hankelvin.github.io/blob/master/_screenshots/word%20embeddings.png">

There are a few word embeddings model available, the original word2vec model was developed in 2013 by Google researchers led by Tomas Mikolov (https://arxiv.org/pdf/1301.3781.pdf), this was followed by the Global Vectors (GloVe) model by Stanford University researchers led by Jeffrey Pennington (https://nlp.stanford.edu/projects/glove/). Finally, the most recent development in this space is the Allen Institute’s Deep Contextualised Word Representations, or ELMo (http://allennlp.org/elmo). 

__Transfer learning using GloVe model trained on a Twitter corpus__ 
I chose to use a transfer learning approach using the GloVe word embeddings model. This was done for a few reasons. There is a GloVe model trained on a Twitter corpus, which is very useful because it better captures all the contextual meanings, language styles and language quirks of Twitter posts (emoticons, typos, information speech) as compared to another model that is trained on a more generic corpus (Wikipedia, Common Crawl etc). By using transfer learning, I also bypass the need to expend large amounts of computing resources to train my own GloVe model. Compared to the word2vec model, GloVe offers lower dimensionality (25, 50, 100 or 200 dimensions) while still retaining strong performance on common NLP tasks including sentiment analysis. The lower dimensionality makes the model more computationally efficient as well as manages the curse of dimensionality problem. 

__Preprocessing__
As the dataset contained tweets in English as well as some in Spanish, and I do not have sufficient Spanish proficiency to work on the Spanish tweets, I had to filter them out from the dataset. To do this without having to manually review each of the tweets, I decided to rely on a simple count of English and Spanish stop words in each tweet. If a tweet has more English stop words than Spanish, I would designate it as English and retain it. To achieve this, I imported the English and Spanish stop words library from the NLTK package, tokenised each tweet and ran all of the tweet’s tokens through both the English and Spanish stop words list. While there could be multilingual tweets that contain both English and Spanish within my dataset, I hypothesised that it would be a small number and would not significantly alter my findings. My code for filtering the tweets by language is as follows: 

￼<img style="-webkit-user-select: none;" src="https://github.com/hankelvin/hankelvin.github.io/blob/master/_screenshots/language%20check.png">

The researchers who created the twitter corpus-trained GloVe model also published their methods for preprocessing Twitter posts (https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb). This is highly important and one of the reasons why I chose the GloVe model. Being able to pre-process my tweets dataset in the same manner as the pre-trained model ensures I am able to enjoy the full capabilities of the pre-trained model. Unfortunately the authors carried out their tweet pre-processing on Ruby, so I had to translate Ruby regex into Python regex in order to write functions to pre-process the tweets in my dataset. 

__Baseline__
As a baseline, I used a basic count vectorising method with logistic regression to classify the tweets. Given my dataset is fairly large, I utilised scikit-learn’s HashingVectoriser and TFIDF functions. The former is to minimise the computational requirements and the latter is to remove most frequent and infrequent tokens in my dataset (as these likely provide very limited signal about the sentiment of the tweet). The number of dimensions in my dataset remained very large (more than a million dimensions), so I used scikit-learn’s TruncatedSVD to reduce the number of dimensions. TruncatedSVD was chosebecause basic Principal Component Analysis (PCA), does not work on sparse matrices and efficacy of SparsePCA is lower for NLP tasks. 

Finally, there is a class imbalance within my dataset, which comprise about 15% negative tweets, 20% neutral tweets and 65% positive tweets. To address this, I resampled by applying Synthetic Minority Over-sampling Technique (SMOTE) in scikit-learn’s imbalanced subpackage. Upsampling was conducted (instead of downsampling) so as to maintain the amount of tweets in the training dataset to provide as much available signal as possible to train the model. 

The accuracy result of from using logistic regression to classify the tweets. It hovers around the 50% mark, which is a good baseline start for a 3-class classification problem. 

￼<img style="-webkit-user-select: none;" src="https://github.com/hankelvin/hankelvin.github.io/blob/master/_screenshots/logreg_tfidf.png">

__Refinement__ 
Given that tweets are short and do not usually have complicated sentence structures, a simple bag of words approach using the Twitter GloVe model would be a good start for the task. The general hypothesis is that tweets of a particular sentiment are generally likely to have more closely related words within them. For example, it is likely to encounter “The new Beatles album is fantastic and it is the best I have heard in years”.  My hypothesis is that “new”, “fantastic”, and “best” would position the tweet towards a part of the vector space of the trained corpus together with other similarly positive-sentiment tweets. I ran the logistic regression model and the results showed an improvement. 

￼<img style="-webkit-user-select: none;" src="https://github.com/hankelvin/hankelvin.github.io/blob/master/_screenshots/logreg_bow.png">

To use the embeddings from the GloVe model, I tokenised the preprocessed version of my tweets dataset and ran search for each token’s vectors in the GloVe Twitter corpus. This resulted in a list of vectors for each tweet. I then summed all the vectors in a tweet and averaged that by the number of tokens. This is effectively a bag of words approach (the summing) with normalisation (the averaging). There are rare instances where a tweet does not have any token that is found in the GloVe Twitter corpus and I imputed a vector of the same size with 0.1 in it. A check of the cosine similar words to this vector indicates it is in a part of the vector space with words that do not hold significant indicators for sentiment (see image below), therefore this approach would not affect the classification significantly. 


__More refinement__
: enhanced bag of words, weighted with parts of speech

￼<img style="-webkit-user-select: none;" src="https://github.com/hankelvin/hankelvin.github.io/blob/master/_screenshots/paravector%20code.png">

￼

computing on the cloud. to use grid search on a number of algorithms

<!--more-->
