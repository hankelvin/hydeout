---
layout: post
title: "DSI3 Capstone - Tweetful of Sentiments"
excerpt_separator: "<!--more-->"
categories:
  - Post Formats
tags:
  - Post Formats
  - readability
  - standard
---

<a href="https://ibb.co/hGWk0y"><img src="https://preview.ibb.co/ewuind/kaomoji.png" alt="kaomoji" border="0"></a>

Updated: 9 July 2018 

__Introduction__ 

For my capstone, I chose to work on sentiment classification of Twitter posts. This was driven by two main factors. Firstly, almost every brand or company has intangible value in their reputation and a large part of the conversation affecting the reputation of brands and companies take place on social media today. Secondly, I thought this would be a nice way to get a good understanding of natural language processing as Twitter posts in particular present a unique challenge in NLP. This is given the informal nature of writing (typos, slang and colloquialism). Given these challenges, I set out with the objective to reach a 65% accuracy in labelling the sentiment of tweets. 

My code for the project can be found <a href="https://github.com/hankelvin/SentimentClassifier"> here, on my GitHub repository. </a>

__Dataset__ 

I used a dataset produced by Spanish researchers  (http://nlp.uned.es/replab2013/) for an academic conference held in 2013. The dataset is prepared by the Natural Language Processing and Information Retrieval Group at UNED. It contains more than 100,000 tweets in English and Spanish, collected over six months for more than 60 entities (across a broad spectrum including companies, brands, universities and music artistes). The UNED researchers got subject matter experts for each domain (e.g. corporate, public agencies etc) to examine each tweet and provide it with a label of Positive, Neutral or Negative. Out of the 100,000 tweets, I extracted 40,000 tweets for training and testing of my models. 

<a href="https://ibb.co/eDEqYJ"><img src="https://preview.ibb.co/gmpPtJ/tweets_dataset.png" alt="tweets dataset" border="0" /></a>


__General approach__ 

Before starting, I conducted extensive research to get an overview of the available tools and approaches for social media sentiment analysis.  This covered articles, book chapters, academic papers, blog and forum posts. A very useful resource is Speech and Language Processing by Stanford University’s Dan Jurafsky and James H. Martin (a draft copy of the upcoming 3rd edition can be found <a href="https://web.stanford.edu/~jurafsky/slp3/">here</a>. 

I identified a few possible approaches that could help me with the sentiment classification - these included bag of words, machine learning, using off-the-shelf NLP packages like Spacy, Textblob etc. In the end, I decided to utilise word embeddings methods as they have become widely utilised in the NLP field. Word embeddings work by vectorising text within a (typically very large) corpus such that all of the tokens in the corpus, given their position in vector space, retain their semantic and syntactic meaning as well as retain the relationships between word pairs. The following diagram is frequently used to illustrate the properties of a word embeddings model. 

<a href="https://imgbb.com/"><img src="https://image.ibb.co/gZquSd/word_embeddings.png" alt="word_embeddings" border="0"></a>

There are a few word embeddings model available, the original word2vec model was developed in 2013 by Google researchers led by Tomas Mikolov <a href="https://arxiv.org/pdf/1301.3781.pdf>[click here]</a>, this was followed by the Global Vectors (GloVe) model by Stanford University researchers led by Jeffrey Pennington <a href="https://nlp.stanford.edu/projects/glove/">[click here]</a>. Finally, the most recent development in this space is the Allen Institute’s Deep Contextualised Word Representations, or ELMo <a href="http://allennlp.org/elmo">[click here]</a>. 

__Transfer learning using GloVe model trained on a Twitter corpus__ 

I chose to use a transfer learning approach using the GloVe word embeddings model. This was done for a few reasons. There is a GloVe model trained on a Twitter corpus, which is very useful because it better captures all the contextual meanings, language styles and language quirks of Twitter posts (emoticons, typos, information speech) as compared to another model that is trained on a more generic corpus (Wikipedia, Common Crawl etc). By using transfer learning, I also bypass the need to expend large amounts of computing resources to train my own GloVe model. Compared to the word2vec model, GloVe offers lower dimensionality (25, 50, 100 or 200 dimensions) while still retaining strong performance on common NLP tasks including sentiment analysis. The lower dimensionality makes the model more computationally efficient as well as manages the curse of dimensionality problem.

__Pre-processing__

As the dataset contained tweets in English as well as some in Spanish, and I do not have sufficient Spanish proficiency to work on the Spanish tweets, I had to filter them out from the dataset. To do this without having to manually review each of the tweets, I decided to rely on a simple count of English and Spanish stop words in each tweet. If a tweet has more English stop words than Spanish, I would designate it as English and retain it. To achieve this, I imported the English and Spanish stop words library from the NLTK package, tokenised each tweet and ran all of the tweet’s tokens through both the English and Spanish stop words list. While there could be multilingual tweets that contain both English and Spanish within my dataset, I hypothesised that it would be a small number and would not significantly alter my findings. My code for filtering the tweets by language is as follows: 

<a href="https://ibb.co/kCxzSd"><img src="https://preview.ibb.co/f3JTLy/language_check.png" alt="language check" border="0" /></a>

The researchers who created the twitter corpus-trained GloVe model also published their methods for pre-processing Twitter posts <a href="https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb
>[click here]</a>. This is highly important and one of the reasons why I chose the GloVe model. Being able to pre-process my tweets dataset in the same manner as the pre-trained model ensures I am able to enjoy the full capabilities of the pre-trained model. Unfortunately the authors carried out their tweet pre-processing on Ruby, so I had to translate Ruby regex into Python regex in order to write functions to pre-process the tweets in my dataset. 

__Baseline__

As a baseline, I used a basic count vectorising method with logistic regression to classify the tweets. As my dataset is fairly large, I utilised scikit-learn’s HashingVectoriser and TFIDF functions. The former is to minimise the computational requirements and the latter is to remove most frequent and infrequent tokens in my dataset (as these likely provide very limited signal about the sentiment of the tweet). The number of dimensions in my dataset remained very large (more than a million dimensions), so I used scikit-learn’s TruncatedSVD to reduce the number of dimensions. TruncatedSVD was chosen because basic Principal Component Analysis (PCA), does not work on sparse matrices and efficacy of SparsePCA is lower for NLP tasks. 

Finally, there is a class imbalance within my dataset, which comprise about 15% negative tweets, 20% neutral tweets and 65% positive tweets. To address this, I resampled by applying Synthetic Minority Over-sampling Technique (SMOTE) in scikit-learn’s imbalanced subpackage. Upsampling was conducted (instead of downsampling) so as to maintain the amount of tweets in the training dataset to provide as much available signal as possible to train the model. 

There is however a risk of overfitting when conducting upsampling (this is because SMOTE works by extrapolating to a random point between two neighbouring samples of the same class). An established method to manage this issue is to use SMOTE in conjunction with Tomek Links removal. This works by identifying samples whose nearest neighbours are not of the same class as themselves and removing these. Effectively, this serves to better demarcate the decision boundary between classes and could help improve classification accuracy. 

The accuracy result of from using logistic regression to classify the tweets. It hovers around the 50% mark, which is a good baseline start for a 3-class classification problem. 

<a href="https://imgbb.com/"><img src="https://image.ibb.co/hXEeSd/logreg_tfidf.png" alt="logreg tfidf" border="0" /></a>

__Refinement__ 

Given that tweets are short and do not usually have complicated sentence structures, a simple bag of words approach using the Twitter GloVe model would be a good start for the task. The general hypothesis is that tweets of a particular sentiment are generally likely to have more closely related words within them. For example, it is likely to encounter “The new Beatles album is fantastic and it is the best I have heard in years”.  My hypothesis is that “new”, “fantastic”, and “best” would position the tweet towards a part of the vector space of the trained corpus together with other similarly positive-sentiment tweets.

I used Gensim to load the GloVe Twitter model. Next, I tokenised the pre-processed version of my tweets dataset and ran search for each token’s vectors in the GloVe Twitter corpus. However, the GloVe Twitter model had a vocabulary size of more than 1.2m. As my dataset contained more than 100,000 tweets, this could mean having to run a search through the entire GloVe vocabulary more than 1m times (assuming on average, that each tweet comprises 10 tokens). This could end up being computationally expensive. To resolve this, I filtered the GloVe Twitter model into 28 separate dictionaries. The splits were principally based on the first character of every token in the GloVe Twitter model. The function I wrote to search for the vectors of each tweet's tokens takes this into account, thereby decreasing search time. 

Once this was all done, each tweet would be represented by a list of vectors. I then summed all the vectors in a tweet and averaged that by the number of tokens. This is effectively a bag of words approach (the summing) with normalisation (the averaging). 

There are rare instances where a tweet does not have any token that is found in the GloVe Twitter corpus and I imputed a vector of the same size with 0.1 in it. A check of the cosine similar words to this vector indicates it is in a part of the vector space with words that do not hold significant indicators for sentiment (see image below), therefore this approach would not affect the classification significantly. 

I ran the logistic regression model and the results showed an improvement. 

<a href="https://imgbb.com/"><img src="https://image.ibb.co/fzkKSd/logreg_bow.png" alt="logreg bow" border="0" /></a>

__More refinement__
Taking into consideration that adjectives and adverbs in a sentence tend to provide the most signal about the sentiment of a sentence (e.g. adjectives such as amazing, terrible and adverbs such as very, frequently). I decided to leverage parts of speech tagging to identify adjectives and adverbs in each tweet and then apply an exponential to effectively give them a higher weightage in the sentence vector. However parts of speech tagging relies significantly on typographical style especially capitalisation of characters, to label the different parts of speech. As such, I had to ensure that the POS tagging is done on my tweets dataset before pre-processing is done. Initially, I tried NLTK's POS function, but the performance was not satisfactory, as such I used Spacy's POS functionality, although the POS tagging was not 100% correct (a result of the informal and loose nature of how tweets are composed), it should be sufficient to improve my model. The following image is the code for how I incorporated the POS tagging to obtain the sentence vectors with higher weightage given to adjectives and adverbs. 

<a href="https://ibb.co/cdes9y"><img src="https://preview.ibb.co/j5BiGd/Screen_Shot_2018_07_01_at_8_30_08_PM.png" alt="Screen Shot 2018 07 01 at 8 30 08 PM" border="0"></a><br>
￼
This time, I also ran a few classifier models useful for detecting labeled classes in clusters (Random Forest Classifier, Stochastic Gradient Descent Classifier and Decision Tree Classifier) and although there was an improvement in the logistic regression model with this new weighted approach, the best performing model is the Random Forest Classifier with accuracy score of slightly above 0.65. 

To further improve the accuracy of the Random Forest Classifier, I also set the class_weight hyperparameter in the algorithm to "balanced". This <a href = "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"> uses the values of y to adjust weights inversely proportional to how frequent a particular class appears in the dataset </a>. In other words, it serves to apply a heavier penalty on a misclassification of the minority class and thereby helping to improve the accuracy of detecting the minority class (negative sentiment tweets in this case).

<a href="https://ibb.co/bQ2HRd"><img src="https://preview.ibb.co/e74Wmd/Screen_Shot_2018_07_01_at_8_12_21_PM.png" alt="Screen Shot 2018 07 01 at 8 12 21 PM" border="0" /></a>

I also ran an AdaBoost (Adaptive Boost) ensemble classifier on top of a Decision Tree Classifier with the best results. AdaBoost works by iteratively training on different parts of the training dataset and taking the points misclassified  (also frequently termed "weak learners") by the underlying classifier, and applying a heavier weight such that it is contained in the training set for the next classifier. Weights are assigned to more accurate classifiers such that these will have a higher "vote" in the final outcome. However, the AdaBoosted-Decision Tree Classifier did not lead to better results compared to a Random Forest Classifier. 

__Analysis and potential next steps__

The best-performing tuned Random Forest Classifier model returned accuracy scores of about 0.78 on the training data set. However, the accuracy of the model on the hold-out test set falls to about 0.65. Despite several tweaks to parameters of the Random Forest Classifier model, the accuracy of the model on the hold-out test set remains within this range, lending support to the likelihood the model is not overfitting. 

A deeper look on the model performance, by examining the classification report, shows that the F1 score is particularly low for the 'Neutral' class (low recall, but high precision i.e. the 'Neutral' class is being predicted in a limited manner, but with a high accuracy when predicted), while the 'Positive' class has a fairly good F1 score. This could have arisen from the imbalanced class distribution (despite the upsampling, the distribution is still about 11.8% : 28.1% : 60.1% for 'Negative', 'Neutral' and 'Positive' classes), and potentially the presence of sarcasm in some of the negative sentiment tweets. 

I have identified a few options to improve the predictive accuracy of the model going forward, these include: 
<ul>
<li> the use of deep learning (Keras or TensorFlow), particularly LSTM-based models that can retain memory of word vectors earlier in the sentence sequence and could be able to more better capture characteristics from turns in literal sentiment across the sentence (such as sarcasm and use of negation such as "not so bad", "not the best" etc). </li>
<li> use of sentiment-focused word embeddings. A group of researchers at the Singapore Management University have developed a word embeddings model, <a href="https://sentivec.preferred.ai/"> SentiVect </a>, that is trained on a corpus comprised of Amazon reviews that have been labelled positive or negative, as well as a lexical dictionary of positive and negative words overlaid during the training of the model. Notably, the paper for the SentiVect embeddings model will be presented at the ACL conference in Melbourne in July 2018. </li>
</ul>

__Conclusion__

Using a mix of packages and approaches, I came up with a bootstrap method for sentiment classification of tweets with good accuracy. I did this by leveraging transfer learning (a GloVe word embeddings model trained on a Twitter corpus) without having to expend significant computing power to train my own model. I was also able to leverage specific functionalities in different NLP packages (NLTK stopwords for language detection, and Spacy's parts of speech tagging) as part of my pre-processing and feature engineering.
<!--more-->
